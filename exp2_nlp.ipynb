{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15b7e40-dc08-48d6-b78c-16e074d746a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "This is a sample text with some extra    spaces and special characters! \n",
      "It's got contractions like can't and I've. \n",
      "There are numbers like 12345 and words like one and two.\n",
      "Stopwords are common words such as the, and, in, to, with.\n",
      "Let's perform text processing on this text!\n",
      "\n",
      "\n",
      "Remove extra white spaces:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Sentence Boundary Detection:\n",
      "This is a sample text with some extra spaces and special characters!\n",
      "It's got contractions like can't and I've.\n",
      "There are numbers like 12345 and words like one and two.\n",
      "Stopwords are common words such as the, and, in, to, with.\n",
      "Let's perform text processing on this text!\n",
      "\n",
      "Expanded Contractions:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Removed Special Characters:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Lowercased Text:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Converted Number Words:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Removed Numbers:\n",
      "this is a sample text with some extra spaces and special characters its got contractions like cannot and ive there are numbers like  and words like  and  stopwords are common words such as the and in to with lets perform text processing on this text\n",
      "\n",
      "Removed Stopwords:\n",
      "['sample', 'text', 'extra', 'spaces', 'special', 'characters', 'got', 'contractions', 'like', 'ive', 'numbers', 'like', 'words', 'like', 'stopwords', 'common', 'words', 'lets', 'perform', 'text', 'processing', 'text']\n",
      "\n",
      "Phrase Extraction:\n",
      "['this is', 'is a', 'a sample', 'sample text', 'text with', 'with some', 'some extra', 'extra spaces', 'spaces and', 'and special', 'special characters', 'characters its', 'its got', 'got contractions', 'contractions like', 'like can', 'can not', 'not and', 'and ive', 'ive there', 'there are', 'are numbers', 'numbers like', 'like and', 'and words', 'words like', 'like and', 'and stopwords', 'stopwords are', 'are common', 'common words', 'words such', 'such as', 'as the', 'the and', 'and in', 'in to', 'to with', 'with lets', 'lets perform', 'perform text', 'text processing', 'processing on', 'on this', 'this text']\n",
      "\n",
      "Tokenization:\n",
      "['this', 'is', 'a', 'sample', 'text', 'with', 'some', 'extra', 'spaces', 'and', 'special', 'characters', 'its', 'got', 'contractions', 'like', 'can', 'not', 'and', 'ive', 'there', 'are', 'numbers', 'like', 'and', 'words', 'like', 'and', 'stopwords', 'are', 'common', 'words', 'such', 'as', 'the', 'and', 'in', 'to', 'with', 'lets', 'perform', 'text', 'processing', 'on', 'this', 'text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sarrvesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarrvesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "    import nltk\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    \n",
    "    # Sample text\n",
    "    text = \"\"\"\n",
    "    This is a sample text with some extra    spaces and special characters! \n",
    "    It's got contractions like can't and I've. \n",
    "    There are numbers like 12345 and words like one and two.\n",
    "    Stopwords are common words such as the, and, in, to, with.\n",
    "    Let's perform text processing on this text!\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Sentence boundary detection\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Expand contractions (you can add more contractions as needed)\n",
    "    contractions_dict = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        # Add more contractions as needed\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Lowercase all text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Convert number words to numeric form (e.g., \"twenty\" to 20)\n",
    "    number_words = {\n",
    "        \"one\": \"1\",\n",
    "        \"two\": \"2\",\n",
    "        \"three\": \"3\",\n",
    "        # Add more number words as needed\n",
    "    }\n",
    "    \n",
    "    for word, number in number_words.items():\n",
    "        text = re.sub(rf'\\b{word}\\b', number, text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Phrase extraction (you can use techniques like n-grams or custom logic)\n",
    "    phrases = [\" \".join(word_tokens[i:i+2]) for i in range(len(word_tokens) - 1)]\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Script Validation (print the results)\n",
    "    \n",
    "    print(\"\\nRemove extra white spaces:\")\n",
    "    print(text)\n",
    "\n",
    "    print(\"\\nSentence Boundary Detection:\")\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "    \n",
    "    print(\"\\nExpanded Contractions:\")\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nRemoved Special Characters:\")\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nLowercased Text:\")\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nConverted Number Words:\")\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nRemoved Numbers:\")\n",
    "    print(text)\n",
    "    \n",
    "    print(\"\\nRemoved Stopwords:\")\n",
    "    print(filtered_text)\n",
    "    \n",
    "    print(\"\\nPhrase Extraction:\")\n",
    "    print(phrases)\n",
    "    \n",
    "    print(\"\\nTokenization:\")\n",
    "    print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
